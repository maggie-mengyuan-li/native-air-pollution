---
title: "Part 1: Cleaning Monitor and Modeled PM2.5 Data"
author: "Maggie Li (ml4424)"
date: "8/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####Load packages
```{r}
library(tidyverse)
library(raster)
```

#PART 1: DATA CLEANING (MONITOR & MODEL)
##1.1 Monitor Data
###Read in and clean EPA 

I downloaded these here: https://aqs.epa.gov/aqsweb/airdata/download_files.html

I've included them in the data folder as csv files since they were a bit of a hassle to clean from their OG .txt file format, but you can try to download them from the source and check!

```{r}
annual_conc_by_monitor_1999 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_1999.csv")
annual_conc_by_monitor_2000 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2000.csv")
annual_conc_by_monitor_2001 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2001.csv")
annual_conc_by_monitor_2002 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2002.csv")
annual_conc_by_monitor_2003 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2003.csv")
annual_conc_by_monitor_2004 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2004.csv")
annual_conc_by_monitor_2005 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2005.csv")
annual_conc_by_monitor_2006 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2006.csv")
annual_conc_by_monitor_2007 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2007.csv")
annual_conc_by_monitor_2008 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2008.csv")
annual_conc_by_monitor_2009 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2009.csv")
annual_conc_by_monitor_2010 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2010.csv")
annual_conc_by_monitor_2011 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2011.csv")
annual_conc_by_monitor_2012 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2012.csv")
annual_conc_by_monitor_2013 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2013.csv")
annual_conc_by_monitor_2014 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2014.csv")
annual_conc_by_monitor_2015 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2015.csv")
annual_conc_by_monitor_2016 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2016.csv")
annual_conc_by_monitor_2017 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2017.csv")
annual_conc_by_monitor_2018 <- read.csv("Data/EPA_data_1999_2018/annual_conc_by_monitor_2018.csv")
```

###Filter out EPA PM2.5 Annual Data
####Make list of all tables to iterate through

```{r}
all_epa_tables <-list(annual_conc_by_monitor_2000,
                      annual_conc_by_monitor_2001,
                      annual_conc_by_monitor_2002,
                      annual_conc_by_monitor_2003,
                      annual_conc_by_monitor_2004,
                      annual_conc_by_monitor_2005,
                      annual_conc_by_monitor_2006,
                      annual_conc_by_monitor_2007,
                      annual_conc_by_monitor_2008,
                      annual_conc_by_monitor_2009,
                      annual_conc_by_monitor_2010,
                      annual_conc_by_monitor_2011,
                      annual_conc_by_monitor_2012,
                      annual_conc_by_monitor_2013,
                      annual_conc_by_monitor_2014,
                      annual_conc_by_monitor_2015,
                      annual_conc_by_monitor_2016,
                      annual_conc_by_monitor_2017,
                      annual_conc_by_monitor_2018)

#empty list to put cleaned data into
cleaned_list <-list()
#assign count variable to iterate through data of years
year <- 1
for (i in 1:length(all_epa_tables)){
  cleaned_list[[year]]<- filter(all_epa_tables[[i]], 
                                Pollutant.Standard == "PM25 24-hour 2012")
  year <- year + 1
}

#cleaned_list contains list items 1-20, corresponding to years 1999-2018. Rows in each table correspond to specific monitor's annual aggregated PM2.5 measurement.
cleaned_list[[1]]
```

####For loop to iterate through all the tables; POC = 1, at least 75% observations for that year, and select for no events or events excluded monitors (at least for now)
```{r}

for (year in 1:19){
  cleaned_list[[year]]$State.Code <- as.integer(as.character(cleaned_list[[year]]$State.Code))
  county_cleaned <- formatC(cleaned_list[[year]]$County.Code, width = 3, format = "d", flag = "0")
  state_cleaned <- formatC(cleaned_list[[year]]$State.Code, width = 2, format = "d", flag = "0")
  cleaned_list[[year]] <- cbind(cleaned_list[[year]],
                                state_cleaned)
  cleaned_list[[year]] <- cbind(cleaned_list[[year]],
                                county_cleaned)
  cleaned_list[[year]] <- unite(data=cleaned_list[[year]],
                                "County",
                                sep = "",
                                c(state_cleaned, county_cleaned),
                                remove = FALSE)
  cleaned_list[[year]]$obs_percent <- cleaned_list[[year]]$Valid.Day.Count/cleaned_list[[year]]$Required.Day.Count
  cleaned_list[[year]] <- filter(cleaned_list[[year]],
                                POC == 1,
                                obs_percent >= 0.75,
                                Event.Type == "No Events" | Event.Type == "Events Excluded") #make sure not to double count monitors because we restrict to POC == 1
}
```

###Read in & clean IMPROVE rural network data

IMPROVE Monitoring Network data downloaded from here: http://views.cira.colostate.edu/fed/QueryWizard/Default.aspx 
I selected EPA PM2.5 MASS FRM (88101) Daily, 2000-2018, and clipped it (PM2_5_1999_2018_rural_clipped.txt). I forgot to download the data the first time with geolocation information so the second imported dataset (IMPROVE_location_tbl) has the lat/lon coordinates, which I joined to the first dataset.

```{r}
PM2_5_1999_2018_rural_clipped<-read.csv("Data/PM2_5_1999_2018_rural_clipped.txt", sep="")
#Aggregate daily to annual data
#Step 1: only keep the year in the date columns, to make it easier to aggregate

#1a: create character vector 'year', to add to data table as a separate column; taking the last 4 characters of the date for the year
year <- str_sub(PM2_5_1999_2018_rural_clipped$Date, -4)
#1b: paste character vector year into datatable
PM2_5_1999_2018_rural_clipped$Year <- paste0(year)
#1c: replace -999 with NAs
#PM2_5_1999_2018_rural_clipped <- PM2_5_1999_2018_rural_clipped  %>% 
#  replace_with_na(replace = list(MF.Value = -999))
PM2_5_1999_2018_rural_clipped$MF.Value <- replace(PM2_5_1999_2018_rural_clipped$MF.Value, 
                                                  which(PM2_5_1999_2018_rural_clipped$MF.Value < 0), NA)
filter(PM2_5_1999_2018_rural_clipped,
       is.na(MF.Value))
#convert date column from factor to date format
PM2_5_1999_2018_rural_clipped$Date <- as.Date(PM2_5_1999_2018_rural_clipped$Date,
                                              "%m/%d/%Y")
PM2_5_1999_2018_rural_clipped <- subset.data.frame(PM2_5_1999_2018_rural_clipped,
                                                   POC == 1)
#new column of days between dates
IMPROVE_interval <- PM2_5_1999_2018_rural_clipped %>% mutate(Between=as.numeric(c(diff(Date),0)))
#aggregate each SiteCode and Year by median days between
IMPROVE_interval <- aggregate(Between ~ SiteCode + Year,
                                    data = IMPROVE_interval,
                                    FUN = median) 
IMPROVE_interval
IMPROVE_interval <- IMPROVE_interval %>% subset(select = c(SiteCode,
                                                           Year,
                                                           Between))
#Compare IMPROVE_interval to number of total observations in each year
IMPROVE_monitor_observations <- PM2_5_1999_2018_rural_clipped %>%
	group_by(SiteCode, Year) %>%
	dplyr::summarise(num_obs = n()) 

#merge the two tables and compare interval with total observations for each monitor and year respectively
#need to make a new column in both that combines monitor name with year as an ID
IMPROVE_monitor_observations$ID <- paste(IMPROVE_monitor_observations$SiteCode,
                                         IMPROVE_monitor_observations$Year)
IMPROVE_interval$ID <- paste(IMPROVE_interval$SiteCode,
                             IMPROVE_interval$Year)

IMPROVE_check_interval <- merge(IMPROVE_monitor_observations,
                                IMPROVE_interval,
                                by = "ID")
#create column with number of days expected (365/Between multiplied by 0.75), to compare with observed
IMPROVE_check_interval$exp_obs <- paste(0.75*365/IMPROVE_check_interval$Between)
IMPROVE_check_interval$exp_obs <- as.integer(IMPROVE_check_interval$exp_obs)

#filter for values that meet 75% of observed threshold
IMPROVE_filtered <- IMPROVE_check_interval %>% filter(num_obs > exp_obs)

#Now join this cleaned list of monitors that meet threshold with original data, which we aggregate first
#also need to make ID column for original data to join on
#seems like aggregate gets rid of NA values automatically and some MF.Value rows are all NAs for certain monitors in certain years
PM2_5_1999_2018_rural_clipped
PM25_rural_annual = aggregate(MF.Value~SiteCode+Year,
                              data = PM2_5_1999_2018_rural_clipped,
                              FUN = mean)
PM25_rural_annual
PM25_rural_annual$ID <- paste(PM25_rural_annual$SiteCode,
                              PM25_rural_annual$Year)

PM25_rural_annual <- merge(PM25_rural_annual,
                           IMPROVE_filtered,
                           by = "ID")
#keep only necessary columns of monitor name, year, and recorded values
PM25_rural_annual <- subset(PM25_rural_annual,
                            select = c(SiteCode, Year, MF.Value))
 
#Join with geolocation table to have lat/lon, COUNTY

# July 7 Note: I was dumb and didn't download the data above with Lat/Long coordinates. So I had to re-download it and join the two datasets.

IMPROVE_location_tbl <- read.csv("Data/IMPROVE_location_tbl.csv")
IMPROVE_lat_lon <- subset(IMPROVE_location_tbl,
                          select = c(2,6,8,9))
#Rename column Code to SiteCode to match other table
colnames(IMPROVE_lat_lon)[colnames(IMPROVE_lat_lon)=="Code"] <- "SiteCode"
#IMPROVE_lat_lon

#Join Tables (final IMPROVE table)
PM25_rural_locations <- merge(x=PM25_rural_annual,
                              y=IMPROVE_lat_lon,
                              by = "SiteCode")
#Note: values range from 1-25 micrograms per meter cubed

#Manually geocode missing county FIPS values, exclude non-US monitors
PM25_rural_locations <- PM25_rural_locations %>% 
  mutate(County = replace(County, 
                          SiteCode=='MAKA2',
                          "53009")) %>% 
  mutate(County = replace(County, 
                          SiteCode=='MAKA1',
                          "53009")) %>% 
  mutate(County = replace(County, 
                          SiteCode=='LOND1',
                          "33015")) %>% 
  mutate(County = replace(County, 
                          SiteCode=='LASU2',
                          "19177")) %>% 
  mutate(County = replace(County, 
                          SiteCode=='HACR1',
                          "15009")) %>%
  filter(SiteCode != "EGBE1",
         SiteCode!= "BYIS1",
         SiteCode!= "BALA1")

#Add leading zero so that Counties have 5 characters total
PM25_rural_locations$County <- as.integer(PM25_rural_locations$County) #convert to integer
county_cleaned <- formatC(PM25_rural_locations$County, width = 5, format = "d", flag = "0")

PM25_rural_locations <- cbind(PM25_rural_locations,
                              county_cleaned)

PM25_rural_locations <- PM25_rural_locations %>% filter(Year != "1999") %>% 
  filter(!str_detect(county_cleaned, "^02")) %>%
  filter(!str_detect(county_cleaned, "^15"))

```

###Monitor PM2.5 table for all years 1999-2018 (join EPA + IMPROVE)

```{r}
#Make a list to store IMPROVE tables by year
IMPROVE_list <- list()
IMPROVE_years <- c(2000:2018)
year <- 1 

for (i in 1:length(IMPROVE_years)){
  IMPROVE_list[[year]] <- subset(PM25_rural_locations, 
                                 Year == IMPROVE_years[i],
                                 select = c(-SiteCode,-County))
  colnames(IMPROVE_list[[year]])[colnames(IMPROVE_list[[year]])=="MF.Value"] <- "annual_mean"
  colnames(IMPROVE_list[[year]])[colnames(IMPROVE_list[[year]])=="county_cleaned"] <- "County"
  year <- year + 1
}

#Do the same for EPA tables
EPA_list <- list()
county_cleaned <- list()
EPA_years <- c(2000:2018)
#start at 2 since we are excluding 1999
for (year in 1:length(EPA_years)){
  EPA_list[[year]] <- data.frame(Year = cleaned_list[[year]]$Year,
                                 annual_mean = cleaned_list[[year]]$Arithmetic.Mean,
                                 Latitude = cleaned_list[[year]]$Latitude,
                                 Longitude = cleaned_list[[year]]$Longitude,
                                 County = cleaned_list[[year]]$County)
}

#Combine EPA and IMPROVE tables for all years
PM25_all <- list()
for (year in 1:19){
  PM25_all[[year]] <- rbind(IMPROVE_list[[year]],
                            EPA_list[[year]])
}
PM25_all
```

##1.2 Model data
###*Optional* Reading in model data from source as raster, extracting values at the county level for all US counties 

This is the part that takes forever to run, you can download all the years' ncdf files from the website and read them in via a loop or one by one (takes about 1-2 hours to run each), and extract county level model PM2.5 using code in this chunk. Tt's commented out because you can skip this and just directly import csv files I've provided, which are the outputs from this.

I've put one ncdf downloaded from the source for 2008 if you want to try running the code, and see if it matches up with my csv file!

```{r}
# Source: Randall Martin's PM2.5 Model http://fizz.phys.dal.ca/~atmos/martin/?page_id=140

# #all counties shapefile
# setwd("/Users/maggieli/Dropbox/Native Air Pollution Paper/Paper/CodeCheck/native-air-pollution/Data/cb_2018_us_county_500k")
# counties_shp <- "cb_2018_us_county_500k.shp"
# all_counties <- st_read(counties_shp, stringsAsFactors = FALSE)
# 
# #group all_counties by state fips, exclude territories and hawaii and alaska
# all_counties <- all_counties %>% arrange(STATEFP) %>%
#   filter(STATEFP != "02",
#          STATEFP != "15")
# unique(all_counties$STATEFP)
# #create state fips key vector
# state_fips <- fips(state.name, county = c())
# 
# #remove hawaii and alaska (FIPS are 15 and 02)
# state_fips <- state_fips[!(state_fips %in% c("02","15"))]
# 
# #save each state's counties as separate sf's, in a list. the function below will iterate through all of these states.
# state_list <- list(length(state_fips))
# for (i in 1:length(state_fips)){
#   state_list[[i]] <- all_counties %>%
#     filter(STATEFP==state_fips[i])
# }
# 
# # read in ncdf as raster stack for example year: 2008
# setwd("/Users/maggieli/Dropbox/Native Air Pollution Paper/Paper/CodeCheck/native-air-pollution/Data/model_PM25")
# model_PM25_2008 <- raster("GWRwSPEC_PM25_NA_200801_200812-RH35.nc")
# model_PM25_2008 <- stack(model_PM25_2008)
# 
# # Make empty list to store model data, iterate through 48 states to extract mean PM2.5 concentrations
# model_PM25_2008_counties <-list()
# 
# # function to extract values from raster into counties shapefile
# extract_model <- function(list, nc, state_list, state_fips){ 
#    list[[i]] <- raster::extract(nc, state_list[[i]], 
#                                 fun=mean, na.rm=TRUE, df=TRUE) #specify function = mean to extract mean concentrations
#    list[[i]]$County <- state_list[[i]]$COUNTYFP
#    list[[i]]$State <- state_fips[i]
#    list[[i]]$FIPS <- paste(list[[i]]$State,list[[i]]$County)
#    list[[i]]$FIPS <- str_replace_all(list[[i]]$FIPS, " ", "")
# }
# extract_model(model_PM25_2008_counties,
#               model_PM25_2008, state_list, state_fips) #state_list and state_fips are already a list and vector created above
```

###Read in county model PM2.5 data
```{r}
# Read in model data into list of length 19 (# study years)

model_PM25 <- list()
DC_model_PM25 <- list()
yr = 2000
for (i in 1:19){
  model_PM25[[i]] <- read_csv(file = paste('Data/model_PM25/model_PM25_', 
                                           yr, '.csv', sep = '')) %>% 
    dplyr::select(c(FIPS, PM25, State))
  DC_model_PM25[[i]] <- read_csv(file = paste('Data/model_PM25/DC_data/DC_modelPM25_', 
                                           yr, '.csv', sep = '')) %>% 
    dplyr::select(c(FIPS, PM25, State)) # join DC data to this!
  
  model_PM25[[i]] <- rbind(model_PM25[[i]], DC_model_PM25[[i]]) %>% 
    mutate(Year = yr)
  yr <- yr + 1
}
model_PM25

```
